{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5043627525607836164\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from scipy.stats import gaussian_kde\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import simulate\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "def add_sequential_indices(dataframe, keymap=None):\n",
    "    if keymap is None:\n",
    "        keymap = {\n",
    "            'IMAGENUMBER': 'IMAGEINDEX', \n",
    "            'PHINUMBER'  : 'PHIINDEX', \n",
    "            'RUN'        : 'RUNINDEX',\n",
    "        }\n",
    "    df = dataframe.reset_index()\n",
    "    for key,newkey in keymap.items():\n",
    "        df[newkey] = np.array(pd.DataFrame(np.sort(np.unique(df[key]))).reset_index().set_index(0).loc[df[key]])\n",
    "    del df['level_0']\n",
    "    return df\n",
    "\n",
    "def pare_data(dataframe):\n",
    "    \"\"\"Remove reflection observations from which gammas cannot be estimated due to missing on or off reflections.\"\"\"\n",
    "    indexnames = dataframe.index.names\n",
    "    df = dataframe.reset_index().set_index(['H', 'K', 'L', 'RUN', 'PHINUMBER'])\n",
    "    df = df.loc[df[df.SERIES.str.contains('on')].index.intersection(df[df.SERIES.str.contains('off')].index)]\n",
    "    if None not in indexnames:\n",
    "        return df.reset_index().set_index(indexnames)\n",
    "    else:\n",
    "        return df.reset_index()\n",
    "\n",
    "def image_metadata(dataframe, keys = None):\n",
    "    \"\"\"Aggregate the image metadata from an integration run into a separate dataframe\"\"\"\n",
    "    if keys is None:\n",
    "        keys = [k for k in dataframe.keys() if 'ipm' in k.lower()]\n",
    "        specifically_check = ['Io', 'BEAMX', 'BEAMY', 'Icryst', 'SERIES', 'RUNINDEX']\n",
    "        for k in specifically_check:\n",
    "            if k in dataframe:\n",
    "                keys.append(k)\n",
    "    return dataframe[['IMAGEINDEX'] + list(keys)].groupby('IMAGEINDEX').mean()\n",
    "\n",
    "def get_raw_gammas(dataframe):\n",
    "    \"\"\"Compute uncorrected intensity ratios, return (raw gamma array, indexing array)\"\"\"\n",
    "    I = pare_data(dataframe)\n",
    "    iobs        = I.pivot_table(values='IOBS', index=['H', 'K', 'L', 'RUNINDEX', 'PHIINDEX'], columns='SERIES', fill_value=np.NaN) \n",
    "    imagenumber = I.pivot_table(values='IMAGEINDEX', index=['H', 'K', 'L', 'RUNINDEX', 'PHIINDEX'], columns='SERIES', fill_value=-1)\n",
    "    gammas = iobs[[i for i in iobs if 'on' in i]].sum(1) / iobs[[i for i in iobs if 'off' in i]].sum(1)\n",
    "    return gammas, imagenumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I = simulate.build_model(\"test/1ubq.pdb.hkl\", \"test/1ubq-flip.pdb.hkl\", multiplicity = 2.0, missing=0.7)\n",
    "I = pd.read_csv(\"test/medium_data.csv\")\n",
    "I = I.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "data = add_sequential_indices(pare_data(I))\n",
    "gammas,imagenumber = get_raw_gammas(data)\n",
    "\n",
    "#h is a dataframe that maps each h,k,l to a unique integer\n",
    "h = gammas.reset_index()[['H','K','L']].drop_duplicates().reset_index(drop=True).reset_index().pivot_table(index=['H','K','L'], values='index')\n",
    "\n",
    "gammaidx = h.loc[gammas.reset_index().set_index(['H', 'K', 'L']).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = len(gammas.reset_index().groupby('RUNINDEX'))\n",
    "M = image_metadata(data)\n",
    "runidx = np.array(M['RUNINDEX'])\n",
    "\n",
    "#We need two sparse tensors to map from Icryst estimates into the liklihood function. \n",
    "tmp = np.array(imagenumber[[i for i in imagenumber if 'on' in i]])\n",
    "idx = np.vstack((np.indices(tmp.shape)[0][tmp > 0], tmp[tmp > 0])).T\n",
    "onimageidx = tf.SparseTensor(idx, np.ones(len(idx), dtype=np.float32), (len(imagenumber), len(M)))\n",
    "onimageidx = tf.sparse_reorder(onimageidx)\n",
    "tmp = np.array(imagenumber[[i for i in imagenumber if 'off' in i]])\n",
    "idx = np.vstack((np.indices(tmp.shape)[0][tmp > 0], tmp[tmp > 0])).T\n",
    "offimageidx = tf.SparseTensor(idx, np.ones(len(idx), dtype=np.float32), (len(imagenumber), len(M)))\n",
    "offimageidx = tf.sparse_reorder(offimageidx)\n",
    "\n",
    "#Constants \n",
    "raw_gammas = tf.constant(np.float32(gammas))\n",
    "ipm        = tf.constant(np.float32(M['Io']))\n",
    "ipm_x      = tf.constant(np.float32(M['IPM_X']))\n",
    "ipm_y      = tf.constant(np.float32(M['IPM_Y']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regularization strength\n",
    "rho = tf.placeholder(tf.float32)\n",
    "\n",
    "#LCs for scaling IPM data\n",
    "x_intercept   = tf.Variable(0.)\n",
    "x_slope       = tf.Variable(100.)\n",
    "y_intercept   = tf.Variable(0.)\n",
    "y_slope       = tf.Variable(100.)\n",
    "ipm_intercept = tf.Variable(0.)\n",
    "ipm_slope     = tf.Variable(1.)\n",
    "\n",
    "#Beam shape\n",
    "sigx          = tf.Variable(10.)\n",
    "sigy          = tf.Variable(10.)\n",
    "\n",
    "#Crystal dimensions\n",
    "xmin          = tf.Variable(-50.*np.ones(r, dtype=np.float32))\n",
    "xmax          = tf.Variable( 50.*np.ones(r, dtype=np.float32))\n",
    "ymin          = tf.Variable(-50.*np.ones(r, dtype=np.float32))\n",
    "ymax          = tf.Variable( 50.*np.ones(r, dtype=np.float32))\n",
    "\n",
    "#gammastimates\n",
    "gamma         = tf.Variable(np.ones(len(h), dtype=np.float32))\n",
    "\n",
    "beamx  = ipm_x * x_slope + x_intercept\n",
    "beamy  = ipm_y * y_slope + y_intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Icryst = 0.25*((ipm_slope**2)*ipm + ipm_intercept) * (\n",
    "    tf.erf((tf.gather(xmin, runidx) - beamx)/sigx) - tf.erf((tf.gather(xmax, runidx) - beamx)/sigx)\n",
    "    ) * (\n",
    "    tf.erf((tf.gather(ymin, runidx) - beamy)/sigy) - tf.erf((tf.gather(ymax, runidx) - beamy)/sigy)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_deviation = (1./len(gammaidx))* \\\n",
    "    tf.reduce_sum(tf.abs(\n",
    "        tf.square(tf.gather(gamma, gammaidx)) - \n",
    "        raw_gammas * tf.sparse_tensor_dense_matmul(offimageidx, tf.expand_dims(Icryst, 1)) / tf.sparse_tensor_dense_matmul(onimageidx, tf.expand_dims(Icryst, 1))\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "regularizer = (1./len(h))*tf.reduce_sum(tf.abs(gamma - 1.))\n",
    "\n",
    "loss = (1. - rho)*absolute_deviation + rho*regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(0.02)\n",
    "train_step = optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "nsteps = 100000\n",
    "verbose=False\n",
    "cmap = plt.get_cmap()\n",
    "norm = plt.Normalize(vmin=1., vmax=nsteps)\n",
    "sm = mpl.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "sm.set_array((1., nsteps))\n",
    "\n",
    "rhoval = .1\n",
    "\n",
    "logvars = {\n",
    "    'xmin': xmin,\n",
    "    'xmax': xmax,\n",
    "    'ymin': ymin,\n",
    "    'ymax': ymax,\n",
    "    'sigx': sigx,\n",
    "    'sigy': sigy,\n",
    "    'ipm_slope' : ipm_slope, \n",
    "    'x_slope' : x_slope, \n",
    "    'y_slope' : y_slope, \n",
    "    'ipm_intercept' : ipm_intercept, \n",
    "    'regularizer' : regularizer,\n",
    "    'absolute_deviation' : absolute_deviation,\n",
    "}\n",
    "\n",
    "L = {k:[] for k in logvars}\n",
    "L['loss'] = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(nsteps):\n",
    "        _, loss_ = sess.run((train_step, loss),feed_dict={rho: rhoval})\n",
    "        if np.isnan(loss_):\n",
    "            break\n",
    "        L['loss'].append(loss_)\n",
    "        gamma_   = np.square(sess.run(gamma, {rho: rhoval}))\n",
    "        x1, x2 = gamma_.min(),gamma_.max()\n",
    "        X = np.linspace(x1 - 0.1*(x2 - x1), x2 + 0.1*(x2 - x1), 200)\n",
    "        plt.plot(X, gaussian_kde(gamma_)(X), c=sm.to_rgba(i+1))\n",
    "        for k,v in logvars.items():\n",
    "            L[k].append(sess.run(v, {rho: rhoval}))\n",
    "        \n",
    "\n",
    "plt.colorbar(sm, ax=plt.gca(), label='Iteration No')\n",
    "plt.xlabel('$\\gamma_{hkl}$')\n",
    "\n",
    "plt.figure()\n",
    "for k,v in L.items():\n",
    "    plt.figure()\n",
    "    plt.plot(v)\n",
    "    plt.ylabel(k)\n",
    "    plt.xlabel('Iteration No')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(data.set_index(['H', 'K', 'L']).loc[h.index]['gamma'].drop_duplicates(), gamma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
